{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text vectorisation: Turning Text into Features\n",
    "\n",
    "# Part 1: N-grams and TF-IDF models \n",
    "\n",
    "More advanced forms of text analysis require that text documents are converted into numerical values or features. In this  section we will examine:\n",
    "\n",
    "* different methods for representing a collection of texts as numbers\n",
    "* the decisions we need to make when generating a particular representation as well as the kinds of insights each numerical representation can give us.\n",
    "\n",
    "We will use tools from the Python libraries `scikit-learn` and `gensim` to perform some popular text vectorisation methods:\n",
    "* Re-cap of N-grams (unigram and bi-gram) term friquency\n",
    "* TF-IDF (Term Frequency–Inverse Document Frequency)\n",
    "* Word embedding—Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "! pip install gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning text into n-grams features \n",
    "### Unigrams\n",
    "\n",
    "Compute the friquency of word occurance using count vectoriser in `scikit-learn`  \n",
    "\n",
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text corpus\n",
    "\n",
    "# Load the parsed news dataset \n",
    "corpus = pd.read_csv('https://raw.githubusercontent.com/valdanchev/SC207/main/sample_news_large_phrased.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "corpus[corpus['query']=='brexit']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset news stories about brexit\n",
    "corpus_brexit = corpus[corpus['query']=='brexit']\n",
    "\n",
    "corpus_toy=corpus_brexit.iloc[[7,22], [1]]\n",
    "\n",
    "# Set the maximum width of columns\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "corpus_toy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to tokenize a collection of text documents and convert \n",
    "# it into a matrix of token counts\n",
    "\n",
    "# Create an instance of the CountVectorizer class\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Learn the vocabulary from the corpus using the toy corpus\n",
    "vectorizer.fit(corpus_toy['title'])\n",
    "\n",
    "# Transform documents to document-term matrix\n",
    "vector = vectorizer.transform(corpus_toy['title'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and \n",
    "# integer feature indices (values) using the vocabulary_ attribute\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that punctuation and single letter's words are removed. We will use below the prerpocessed tokens you have already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the feature index of a token\n",
    "vectorizer.vocabulary_.get('block')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers assigned to each token (e.g., \"brexit\") are indices. For clarity, indices are sorted in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the document-term matrix of rows (documents) and \n",
    "# columns (count for the number of times a token appeared in the document) \n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vector.toarray()` returns a matrix where the rows indicate the number of documents (two in our case) and the columns indicate the size of the vocabulary of the entire corpus (all documents).\n",
    "\n",
    "Each document is encoded as a vector with a length indicating the size of the vocabulary of the entire corpus and an integer count for the number of times each token appeared in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary of terms (keys) and indices (values) in the feature matrix by values in ascending order\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "\n",
    "# Print the document-term matrix\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of 24 unigram features. The 1st token `brexit` has appeared twice in the first title and once in the second title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find (1) the most friquent token in a document, (2) the number of times it appears in that document \n",
    "# and (3) the document in which it appears\n",
    "maximum = vector.toarray().max()\n",
    "index_of_maximum = np.where(vector.toarray() == maximum)\n",
    "\n",
    "print(\"max:\", maximum)\n",
    "print(\"index:\", index_of_maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the vector of integer count in ascending order\n",
    "np.sort(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using the entire data set of News Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "vectorizer_corpus = CountVectorizer()\n",
    "\n",
    "#  Learn the vocabulary from the corpus and tokenise\n",
    "vectorizer_corpus.fit(corpus['text'])\n",
    "\n",
    "# Transform documents to document-term matrix\n",
    "vector_corpus = vectorizer_corpus.transform(corpus['text'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using the vocabulary_ attribute\n",
    "print(dict(sorted(vectorizer_corpus.vocabulary_.items(), key=lambda item: item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the document-term matrix\n",
    "print(vector_corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of vector_corpus.toarray(), i.e., number of rows and columns\n",
    "vector_corpus.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Using the entire corpus, find (1) the most friquent token in a document, (2) the number of times it appears in that document and (3) the document in which it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write below the code for Exercise 1\n",
    "\n",
    "maximum = vector_corpus.toarray().max()\n",
    "index_of_maximum = np.where(vector_corpus.toarray() == maximum)\n",
    "\n",
    "print(\"max:\", maximum)\n",
    "print(\"token index:\", index_of_maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the most frequent token is in document 3 and indexed 12823. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the token indexed 12823 by getting a key in a dictionary by its value \n",
    "# The value in the \"vectorizer_corpus.vocabulary_\" is the token index\n",
    "\n",
    "dict((v,k) for k,v in vectorizer_corpus.vocabulary_.items())[12823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To double check, get value by key\n",
    "\n",
    "vectorizer_corpus.vocabulary_.get('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-grams (combination of two tokens)\n",
    "In the unigram transformation, each token is a feature. For example, `general` and `election` are two separate features. The bi-gram transformation relaxes this contrain by pairing each word to previous and subsequent words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unigrams and bigrams\n",
    "    # ngram_range of (1, 1) extracts unigrams\n",
    "    # ngram_range of (1, 2) extracts unigrams and bigrams\n",
    "    # ngram_range of (2, 2) extracts only bigrams\n",
    "\n",
    "# Create an instance of the CountVectorizer class set bigram extraction   \n",
    "vectorizer = CountVectorizer(ngram_range=(1,2))\n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "vectorizer.fit(corpus_toy['title'])\n",
    "\n",
    "# Transform documents to document-term matrix\n",
    "vector = vectorizer.transform(corpus_toy['title'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using vocabulary_\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "\n",
    "# Print the document-term matrix\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of 28 bigram-based features. The count is either 1 or 0 for each of our bigram.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Term frequency–inverse document frequency (TF-IDF)\n",
    "\n",
    "TF-IDF vectorisation weights down tokens that are present across many documents in the corpus (in particular, words like \"of\" and \"the\" if stop words are not removed) and are therefore less informative than tokens that are present in specific documents in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's first get the `TF` (term frequency) as before "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the CountVectorizer function we used above to count n-grams\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_toy['title'])\n",
    "vector = vectorizer.transform(corpus_toy['title'])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's now compute the `IDF` part\n",
    "\n",
    "IDF = log(N + 1 / n + 1) + 1 where N is the total number of documents and n is the number of documents in which the term appears; constant “1” is added to the numerator and denominator to prevent zero divisions (see [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "matrix = vectorizer.fit_transform(corpus_toy['title'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using vocabulary_\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "\n",
    "# Print the IDF scores \n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF for the term 'block'\n",
    "import math as m\n",
    "m.log((2+1)/(1+1))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "Compute the IDF for the term 'uk'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we get the TF-IDF for our toy corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TF-IDF matrix into a DataFrame   \n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is TF-IDF computed by `scikit-learn`?  \n",
    "\n",
    "\n",
    "TF-IDF(t,d) = TF * IDF\n",
    "\n",
    "What is the TF-IDF of the term 'brexit' which is term 1 in document 0 so TF-IDF(1,0)\n",
    "\n",
    "TF = 2\n",
    "\n",
    "IDF = log(N + 1 / n + 1) + 1 where N is the total number of documents and n is the number of documents in which the term appears; constant “1” is added to the numerator and denominator to prevent zero divisions (see [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the term \"brexit\" is present in two of two documents\n",
    "IDF = m.log((2+1)/(2+1))+1 \n",
    "IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So TF-IDF for term 1 (brexit) in document 0 is **TF-IDF (1,0) = TF * TDF = 2 * 1 = 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try another example, the fourth term ('election') in document 0\n",
    "\n",
    "TF-IDF(4.0) = TF * IDF\n",
    "\n",
    "TF = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the term \"election\" is present in one of two documents\n",
    "IDF = m.log((2+1)/(1+1))+1\n",
    "IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So TF-IDF for term 4 ('election') in document 0 is **TF-IDF (4,0) = TF * TDF = 1 * 1.405 = 1.405**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above TF-IDF matrix is not normalised. Typically, it is recommended that the TF-IDF weights are normalised meaning that the weights in the matrix will range between 0 and 1. Below is the normalisation code (L2 normalisation is default in the TfidfVectorizer function but we indicate it below for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(norm ='l2')\n",
    "\n",
    "# Learn the vocabulary from the corpus and create a document-term matrix\n",
    "matrix = vectorizer.fit_transform(corpus_toy['title'])\n",
    "\n",
    "# Convert the TF-IDF matrix into a DataFrame\n",
    "pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorisation of the 'raw' news sub-corpus related to Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Convert our corpus of raw documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Learn the vocabulary from the corpus and create a document-term matrix\n",
    "matrix = vectorizer.fit_transform(corpus_brexit['text'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using vocabulary_\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the IDF scores\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF of a few tokens in the brexit corpus\n",
    "print(\"IDF score of the term 'the':\",vectorizer.idf_[vectorizer.vocabulary_[\"the\"]])\n",
    "print(\"IDF score of the term 'brexit':\",vectorizer.idf_[vectorizer.vocabulary_[\"brexit\"]])\n",
    "print(\"IDF score of the term 'deal':\",vectorizer.idf_[vectorizer.vocabulary_[\"deal\"]])\n",
    "print(\"IDF score of the term 'protesters':\", vectorizer.idf_[vectorizer.vocabulary_[\"protesters\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word `\"the\"` is present in many documents and hence the vector value is close to 1; Converseley, the term `\"protesters\"` is present in few documents and has a higher IDF value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# TF-IDF matrix\n",
    "# The vectorizer.get_feature_names() gives you the list of feature names\n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"the\" in the brexit corpus\n",
    "tf_idf_df.loc[:,['the','brexit','deal','protesters']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token `\"the\"` is downweighted but still has high TF-IDF weights due to the high term frequency (Note that the TF-IDF score is a product of term frequency & inverse document frequency). The term `\"protesters\"` is present in a few documents and because it's term frequency is 0 in many documents, the TF-IDF score is 0 too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore some parameters of the TfidfVectorizer function \n",
    "As with other functions, you can use Shift + Tab to explore the parameters\n",
    "\n",
    "`stop_words` removes stopwords, only for english, some with issues; automatically filters stop words based on intra corpus document frequency of terms \n",
    "\n",
    "`min_df` ignores terms that have a document frequency lower than the given threshold (float or int, default=1.0)\n",
    "\n",
    "`max_df` ignores terms that have a document frequency higher than the given threshold (float or int, default=1.0.)\n",
    "\n",
    "`max_features` default=None, if not None, build a vocabulary that only consider the top features ordered by term frequency across the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our corpus of row documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             min_df = 0.2, \n",
    "                             max_df = 0.9) # threshold depends on corpus and question\n",
    "                             # max_features=5\n",
    "    \n",
    "# Learn the vocabulary from the corpus and create a document-term matrix\n",
    "matrix = vectorizer.fit_transform(corpus_brexit['text'])\n",
    "\n",
    "# Summarize & print the tokens and the matrix of TF-IDF features\n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectorisation using the `tokenised` News sub-corpus related to Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF on your tokenised news corpus related to Brexit\n",
    "            \n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             min_df = 0.2, \n",
    "                             max_df = 0.9) # threshold depends on corpus and question\n",
    "                             # max_features = 5 # you can specify a subset of features to consider\n",
    "\n",
    "# Learn the vocabulary from the corpus and create a document-term matrix\n",
    "matrix = vectorizer.fit_transform(corpus_brexit['tokens'])\n",
    "\n",
    "# Create a DataFrame \n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the word `\"the\"` appears in more than 90% of the documents and is removed on that basis. Also, the word `\"protesters\"` appears in less than 20% of the documents and is removed on that basis.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the TF-IDF vectors for a few tokens\n",
    "# Error message indicating tokens not in our corpus due to the thresholding we performed\n",
    "# tf_idf_df.loc[:,['the','brexit','deal','protesters']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only tokens that are in the tf_idf_df DataFrame\n",
    "tf_idf_df.loc[:,['brexit','deal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot two features using a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create figure and set figure size\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.figure(figsize = (15,10))\n",
    "\n",
    "# Create scatterplot — alpha controls the transparency and s controls the size of markers\n",
    "fig = sns.scatterplot(data=tf_idf_df, x='brexit', y='deal', alpha=0.4, s=600, color = 'm')\n",
    "# fig.set_xlabel(\"Brexit\")\n",
    "# fig.set_ylabel(\"Deal\")\n",
    "\n",
    "# Add label for each point\n",
    "for line in range(0,tf_idf_df.shape[0]):\n",
    "    fig.text(tf_idf_df.brexit[line], tf_idf_df.deal[line], tf_idf_df.index[line], \n",
    "             horizontalalignment='center', size='small', color='black', weight='light') # possibly add fontsize=15"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the figure above, dots are documents and dot labels indicate the ID of the document in the index column in the DataFrame—if you have more interpretable labels, you could easily plot them instead of the index. You can identify from the figure, for example, the documents that focus on the word 'deal', including documents 11, 24, 12, 3, 0, and 21. While all documents are related to Brexit, the TF-IDF score for brexit is low for some documents due to the fact that the word brexit was not mentioned in the text of those document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the 25 docuemtns about Brexit using scikit-learn's implementations of [Principal Component Analysis (PCA)](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) and [K-means clustering](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html)\n",
    "\n",
    "#### Principal Component Analysis (PCA)\n",
    "The above analysis visualises documents with respect to only two tokens/features/vectors. To visualise documents with respect to all tokens, we will apply a useful technique for dimensionality reduction called Principal Component Analysis (PCA). PCA takes multidimensional data and projects each data point in the sample into few components (we will use the first two components) that preserve as much as possible the variance in the data. For more information about PCA and how to implement it in Python, read [here](http://www.textbook.ds100.org/ch/25/pca_dims.html).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Principal Component Analysis\n",
    "\n",
    "# Initialise the PCA estimator and keep the first 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA estimator; first convert the sparse matrix to an array using toarray \n",
    "pca_components=pca.fit_transform(matrix.toarray())\n",
    "pca_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### K-means clustering\n",
    "Clustering is an approach that aims to group a set of observations into subgroups or clusters (without any prior information about cluster membership) such that observations assigned to the same cluster are more similar to each other than those in other clusters. We will employ the _k_-means clustering algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the k-means estimator with 3 clusters\n",
    "kmeans = KMeans(n_clusters=3)\n",
    "\n",
    "# Fit the k-means estimator using the two components \n",
    "kmeans.fit(pca_components)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the cluster variable as a column in the tf_idf_df variable\n",
    "tf_idf_df['cluster'] = kmeans.labels_\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a document to a category \n",
    "tf_idf_df['category'] = kmeans.labels_\n",
    "tf_idf_df['pca_components_1'] = pca_components[:, 0]\n",
    "tf_idf_df['pca_components_2'] = pca_components[:, 1]\n",
    "\n",
    "# Set figure size\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.figure(figsize = (11.7,8.27))\n",
    "\n",
    "# Scatterplot with the 1st principal component on the horizontal x axes and 2nd principal component on the vertical y axis\n",
    "fig = sns.scatterplot(x = pca_components[:, 0], y = pca_components[:, 1], hue=kmeans.labels_, alpha=0.8, s=200)\n",
    "\n",
    "# This for loop assign country name to each data point iteratively\n",
    "for line in range(0,tf_idf_df.shape[0]):\n",
    "     fig.text(pca_components[line,0]+0.015, pca_components[line,1], # where the labels should be positioned\n",
    "     tf_idf_df.index[line], # add labels to each data point \n",
    "     horizontalalignment='left', size='small', color='black', weight='light') # possibly add fontsize=10\n",
    "\n",
    "# Add labels to the horisontal x axis and vertical y axis\n",
    "labels = fig.set(xlabel='1st principal component', ylabel='2nd principal component')\n",
    "\n",
    "# Add title 'Cluster' to the legend and locate it in the upper right of the plot\n",
    "legend = plt.legend(title='Cluster', loc='upper right')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the TF-IDF for the entire corpus using Principal Component Analysis and K-means clustering  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             min_df = 0.1, \n",
    "                             max_df = 0.9, # threshold depends on corpus and question\n",
    "                             max_features=100) \n",
    "matrix = vectorizer.fit_transform(corpus['tokens'])\n",
    "\n",
    "# DataFrame\n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Repeat the Principal Component Analysis workflow\n",
    "\n",
    "# Initialise the PCA estimator with 2 components\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit the PCA estimator; first convert the sparse matrix to an array using toarray \n",
    "pca_components=pca.fit_transform(matrix.toarray())\n",
    "pca_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we know how many clusters to form? \n",
    "We can learn the optimal number of clusters for our data authomatically. We run the k-means algorithm with various values of _k_ and plot each value of _k_ against the sum of squared distances between each data point (document) and its cluster centre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sum_of_squared_errors = [] # Initialise a list\n",
    "\n",
    "K = range(1,31)\n",
    "for k in K:\n",
    "  kmeans = KMeans(n_clusters=k)\n",
    "  kmeans.fit(pca_components)\n",
    "  Sum_of_squared_errors.append(kmeans.inertia_)   \n",
    "\n",
    "Sum_of_squared_errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot k against the sum of squared distances\n",
    "We perform multiple runs of the k-means clustering algorithm, and the plot below shows how the sum of squared distances varies with values of _k_ between 1 and 30. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot appearance and size\n",
    "sns.set(rc={'figure.figsize':(8.2,5.8)})\n",
    "sns.set_context(\"notebook\", font_scale=1.5, rc={\"lines.linewidth\": 2.5})\n",
    "\n",
    "# Generate the plot\n",
    "fig = sns.lineplot(x= K, y = Sum_of_squared_errors)    \n",
    "\n",
    "# Add x and y labels\n",
    "labels = fig.set(xlabel='Number of clusters, k', ylabel='Total squared distances')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The total squared distances decreases slowly after _k_ in the range 4 to 6. We run our k-means algorithm on the entire dataset with _k_ = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the k-means estimator with 3 clusters\n",
    "kmeans = KMeans(n_clusters=4)\n",
    "\n",
    "# Fit the k-means estimator using the two components \n",
    "kmeans.fit(pca_components)\n",
    "kmeans.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a document to a category \n",
    "tf_idf_df['category'] = kmeans.labels_\n",
    "tf_idf_df['pca_components_1'] = pca_components[:, 0]\n",
    "tf_idf_df['pca_components_2'] = pca_components[:, 1]\n",
    "\n",
    "# Set figure size\n",
    "sns.set_context(\"notebook\", font_scale=1.5)\n",
    "plt.figure(figsize = (11.7,8.27))\n",
    "\n",
    "# Scatterplot with the 1st principal component on the horizontal x axes and 2nd principal component on the vertical y axis\n",
    "fig = sns.scatterplot(x = pca_components[:, 0], y = pca_components[:, 1], hue=kmeans.labels_, alpha=0.8, s=200)\n",
    "\n",
    "# This for loop assign country name to each data point iteratively\n",
    "for line in range(0,tf_idf_df.shape[0]):\n",
    "     fig.text(pca_components[line,0]+0.015, pca_components[line,1], # where the labels should be positioned\n",
    "     tf_idf_df.index[line], # add labels to each data point \n",
    "     horizontalalignment='left', size='small', color='black', weight='light') # possibly add fontsize=10\n",
    "\n",
    "# Add labels to the horisontal x axis and vertical y axis\n",
    "labels = fig.set(xlabel='1st principal component', ylabel='2nd principal component')\n",
    "\n",
    "# Add title 'Cluster' to the legend and locate it in the upper right of the plot\n",
    "legend = plt.legend(title='Cluster', loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Use the TF-IDF matrix to compute the cosine similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import cosine_similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity # Generate cosine similarity matrix\n",
    "cosine_sim = cosine_similarity(matrix, matrix)  \n",
    "cosine_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to a DataFrame\n",
    "print(len(cosine_sim))\n",
    "cosine_sim_list  = cosine_sim.tolist()\n",
    "cosine_sim_df = pd.DataFrame.from_records(cosine_sim_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize = (30,30))\n",
    "sns.heatmap(cosine_sim_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "1. [Converting Text to Features,](https://learning.oreilly.com/library/view/natural-language-processing/9781484242674/html/475440_1_En_3_Chapter.xhtml#) in _Natural Language Processing Recipes_. Akshay Kulkarni & Adarsha Shivananda. 2019.\n",
    "2. [Sklearn's module on feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html).\n",
    "3. [Vector Semantics and Embeddings,](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in _Speech and Language Processing_. Daniel Jurafsky & James H. Martin. Draft of December 30, 2020.\n",
    "4. [K-Means Clustering with scikit-learn.](http://jonathansoma.com/lede/algorithms-2017/classes/clustering/k-means-clustering-with-scikit-learn/)\n",
    "5. [Pandas for Everyone.](https://www.pearson.com/us/higher-education/program/Chen-Pandas-for-Everyone-Python-Data-Analysis/PGM335102.html). Daniel Chen. 2018. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
