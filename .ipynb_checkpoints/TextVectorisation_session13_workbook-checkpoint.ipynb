{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text vectorisation: Turning Text into Features\n",
    "\n",
    "More advanced forms of text analysis require that text documents are converted into numerical values or features. In this  section we will examine:\n",
    "\n",
    "* different methods for representing a collection of texts as numbers\n",
    "* the decisions we need to make when generating a particular representation as well as the kinds of insights each numerical representation can give us.\n",
    "\n",
    "We will use tools from the Python libraries `scikit-learn` and `gensim` to perform some popular text vectorisation methods:\n",
    "* Re-cap of N-grams (unigram and bi-gram) term friquency\n",
    "* TF-IDF (Term Frequency–Inverse Document Frequency)\n",
    "* Word embedding—Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning text into n-grams features \n",
    "### Unigrams\n",
    "\n",
    "Compute the friquency of word occurance using count vectoriser in `scikit-learn`  \n",
    "\n",
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text corpus\n",
    "\n",
    "# Load the parsed news dataset \n",
    "corpus = pd.read_csv('sample_news_large_phrased.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset news stories about brexit\n",
    "corpus_brexit = corpus[corpus['query']=='brexit']\n",
    "\n",
    "corpus_toy=corpus_brexit.iloc[[7,22], [1]]\n",
    "\n",
    "# Set the maximum width of columns\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "corpus_toy.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to tokenize a collection of text documents and convert it into a matrix of token counts\n",
    "\n",
    "# Create an instance of the CountVectorizer class\n",
    "\n",
    "\n",
    "# Learn the vocabulary from the corpus using the toy corpus\n",
    "\n",
    "\n",
    "# encode documents as vectors\n",
    "\n",
    "\n",
    "# The vocabulary_ attribute maps the tokens (keys) to the integer feature indices (values) in a dictionary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that punctuation and single letter's words are removed. We will use below the prerpocessed tokens you have already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the feature index of a token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers assigned to each token (e.g., \"brexit\") are indices. For clarity, indices are sorted in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the matrix of rows (documents) and columns (count for the number of times a token appeared in the document) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vector.toarray()` returns a matrix where the rows indicate the number of documents (two in our case) and the columns indicate the size of the vocabulary of the entire corpus (all documents).\n",
    "\n",
    "Each document is encoded as a vector with a length indicating the size of the vocabulary of the entire corpus and an integer count for the number of times each token appeared in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary of terms (keys) and indices (values) in the feature matrix by values in ascending order\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of 24 unigram features. The 1st token `brexit` has appeared twice in the first title and once in the second title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the most friquent token in the corpus and the number of times it appeared in the corpus \n",
    "maximum = vector.toarray().max()\n",
    "index_of_maximum = np.where(vector.toarray() == maximum)\n",
    "\n",
    "print(\"max:\", maximum)\n",
    "print(\"index:\", index_of_maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the vector of integer count in ascending order\n",
    "np.sort(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using the entire data set of News Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "vectorizer_corpus = CountVectorizer()\n",
    "\n",
    "#  Learn the vocabulary from the corpus and tokenise\n",
    "vectorizer_corpus.fit(corpus['text'])\n",
    "\n",
    "# encode documents as vectors\n",
    "vector_corpus = vectorizer.transform(corpus['text'])\n",
    "\n",
    "# summarize & generate output\n",
    "print(vectorizer_corpus.vocabulary_)\n",
    "print(vector_corpus.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "For the entire corpus, find the most friquent token in the corpus and the number of times it appeared in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write below the code for Exercise 1\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-grams (combination of two tokens)\n",
    "In the unigram transformation, each token is a feature. For example, `general` and `election` are two separate features. The bi-gram transformation relaxes this contrain by pairing each word to previous and subsequent words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unigrams and bigrams\n",
    "    # ngram_range of (1, 1) extracts unigrams\n",
    "    # ngram_range of (1, 2) extracts unigrams and bigrams\n",
    "    # ngram_range of (1, 2) extracts bigrams\n",
    "\n",
    "# Create an instance of the CountVectorizer class set bigram extraction   \n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "\n",
    "# encode documents as vectors\n",
    "\n",
    "# # The vocabulary_ attribute maps the tokens (keys) to the integer feature indices (values) in a dictionary\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of 28 bigram-based features. The count is either 1 or 0 for each of our bigram.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Term frequency–inverse document frequency (TF-IDF)\n",
    "\n",
    "TF-IDF vectorisation weights down tokens that are present across many documents in the corpus (in particular, words like \"of\" and \"the\" if stop words are not removed) and are therefore less informative than tokens that are present in specific documents in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "\n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "\n",
    "\n",
    "# Summarize & print the tokens and the matrix of TF-IDF features \n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### How is TF-IDF computed by `scikit-learn`?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF \n",
    "# IDF = log(1 + N/ 1 + n) + 1 \n",
    "# N is the total number of documents \n",
    "# n is the number of documents in which the word appears\n",
    "# constant “1” is added to the numerator and denominator to prevent zero divisions\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html\n",
    "    \n",
    "import math as m\n",
    "# the term \"brexit\" is present in two of two documents\n",
    "m.log((2+1)/(2+1))+1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the term \"election\" is present in one of two documents\n",
    "m.log((2+1)/(1+1))+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectorisation of the `row` news sub-corpus related to Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our corpus of row documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "vectorizer.fit(corpus_brexit['text'])\n",
    "\n",
    "# Summarize & print the tokens and the matrix of TF-IDF features\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"the\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'the':\",vectorizer.idf_[vectorizer.vocabulary_[\"the\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"brexit\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'brexit':\",vectorizer.idf_[vectorizer.vocabulary_[\"brexit\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"deal\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'deal':\",vectorizer.idf_[vectorizer.vocabulary_[\"deal\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"protesters\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'protesters':\", vectorizer.idf_[vectorizer.vocabulary_[\"protesters\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word `\"the\"` is present in many documents and hence the vector value is close to 1; Converseley, the term `\"protesters\"` is present in few documents and has a higher vector value. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's explore some parameters of the TfidfVectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key parameters of the TfidfVectorizer function\n",
    "    # min_df: float or int, default=1.0. ignores terms that have a document frequency lower than the given threshold\n",
    "    # max_df: float or int, default=1.0. ignores terms that have a document frequency higher than the given threshold\n",
    "    # stop_words: removes stopwords, only for english, with issues; max_df set to a value in the range [0.7, 1.0) \n",
    "    # automatically filters stop words based on intra corpus document frequency of terms.\n",
    "\n",
    "# Convert our corpus of row documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(stop_words='english', min_df = 0.2, max_df = 0.9) # threshold depends on corpus and question\n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "vectorizer.fit(corpus_brexit['text'])\n",
    "\n",
    "# Summarize & print the tokens and the matrix of TF-IDF features\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"the\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'the':\",vectorizer.idf_[vectorizer.vocabulary_[\"the\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word `\"the\"` appears in more than 90% of the documents and is removed on that basis.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"brexit\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'brexit':\",vectorizer.idf_[vectorizer.vocabulary_[\"brexit\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"deal\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'deal':\",vectorizer.idf_[vectorizer.vocabulary_[\"deal\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"election\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'election':\", vectorizer.idf_[vectorizer.vocabulary_[\"protesters\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word `\"protesters\"` appears in less than 20% of the documents and is removed on that basis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectorisation using the `tokenied` News sub-corpus related to Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english', min_df = 0.2, max_df = 0.9) # threshold depends on corpus and question\n",
    "#Tokenize and build vocab\n",
    "vectorizer.fit(corpus_brexit['tokens'])\n",
    "#Summarize\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"the\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'the':\",vectorizer.idf_[vectorizer.vocabulary_[\"the\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"brexit\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'brexit':\", vectorizer.idf_[vectorizer.vocabulary_[\"brexit\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the collocation \"deal\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'deal':\", vectorizer.idf_[vectorizer.vocabulary_[\"deal\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the collocation \"prime_minister\" in the brexit corpus\n",
    "print(\"TF-IDF score of the term 'prime_minister':\", vectorizer.idf_[vectorizer.vocabulary_[\"prime_minister\"]])\n",
    "print(\"Mean TF-IDF in corpus:\", np.mean(vectorizer.idf_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings and word2vec\n",
    "\n",
    "> You shall know a word by the company it keeps (Firth, 1957).\n",
    "\n",
    "`Word2vec` [Mikolov et al. 2013](https://arxiv.org/abs/1301.3781) and related techniques (e.g., [GloVe](https://nlp.stanford.edu/projects/glove/)) use the context of a given word — i.e., the words surrounding a word — to learn its meaning and represent it as vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your tokens in the News dataset into a list\n",
    "corpus_brexit['tokens']= corpus_brexit['tokens'].apply(lambda token_string: token_string.split('|*|'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.decomposition import PCA\n",
    "from matplotlib import pyplot\n",
    "\n",
    "# training the word2vec model\n",
    "skipgram = Word2Vec(corpus_brexit['tokens'], size =300, window = 3, min_count=1,sg = 1)\n",
    "\n",
    "print(\"Dimensionality—size of vocabulary and size of vectors:\", skipgram)\n",
    "\n",
    "# access vector for one word, \"brexit\" in this instance\n",
    "print(\"vectors for 'brexit':\", skipgram['brexit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.wv.similarity('brexit', 'migration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.wv.most_similar(positive = \"brexit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Principal component analysis (PCA) on the skipgram model output and plot the first 2 components\n",
    "\n",
    "data = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(data)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(28,20))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "       plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Akshay Kulkarni and Adarsha Shivananda. 2019. Natural Language Processing Recipes. [Chapter 3: Converting Text to Features](https://learning.oreilly.com/library/view/natural-language-processing/9781484242674/html/475440_1_En_3_Chapter.xhtml#)\n",
    "\n",
    "2. [Sklearn's module on feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
