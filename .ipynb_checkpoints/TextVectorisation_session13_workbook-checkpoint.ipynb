{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text vectorisation: Turning Text into Features\n",
    "\n",
    "More advanced forms of text analysis require that text documents are converted into numerical values or features. In this  section we will examine:\n",
    "\n",
    "* different methods for representing a collection of texts as numbers\n",
    "* the decisions we need to make when generating a particular representation as well as the kinds of insights each numerical representation can give us.\n",
    "\n",
    "We will use tools from the Python libraries `scikit-learn` and `gensim` to perform some popular text vectorisation methods:\n",
    "* Re-cap of N-grams (unigram and bi-gram) term friquency\n",
    "* TF-IDF (Term Frequency–Inverse Document Frequency)\n",
    "* Word embedding—Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "\n",
    "! pip install gensim\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turning text into n-grams features \n",
    "### Unigrams\n",
    "\n",
    "Compute the friquency of word occurance using count vectoriser in `scikit-learn`  \n",
    "\n",
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text corpus\n",
    "\n",
    "# Load the parsed news dataset \n",
    "corpus = pd.read_csv('sample_news_large_phrased.csv', index_col='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset news stories about brexit\n",
    "corpus_brexit = corpus[corpus['query']=='brexit']\n",
    "\n",
    "corpus_toy=corpus_brexit.iloc[[7,22], [1]]\n",
    "\n",
    "# Set the maximum width of columns\n",
    "pd.options.display.max_colwidth = 200\n",
    "\n",
    "corpus_toy.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use CountVectorizer to tokenize a collection of text documents and convert it into a matrix of token counts\n",
    "\n",
    "# Create an instance of the CountVectorizer class\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Learn the vocabulary from the corpus using the toy corpus\n",
    "vectorizer.fit(corpus_toy['title'])\n",
    "\n",
    "# Transform documents to document-term matrix\n",
    "vector = vectorizer.transform(corpus_toy['title'])\n",
    "\n",
    "# The vocabulary_ attribute gives you a dictionary with tokens (keys) and integer feature indices (values)\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that punctuation and single letter's words are removed. We will use below the prerpocessed tokens you have already preprocessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access the feature index of a token\n",
    "vectorizer.vocabulary_.get('brexit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The numbers assigned to each token (e.g., \"brexit\") are indices. For clarity, indices are sorted in the cell bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the document-term matrix of rows (documents) and columns (count for the number of times a token appeared in the document)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`vector.toarray()` returns a matrix where the rows indicate the number of documents (two in our case) and the columns indicate the size of the vocabulary of the entire corpus (all documents).\n",
    "\n",
    "Each document is encoded as a vector with a length indicating the size of the vocabulary of the entire corpus and an integer count for the number of times each token appeared in the document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the dictionary of terms (keys) and indices (values) in the feature matrix by values in ascending order\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "\n",
    "# Print the document-term matrix\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of 24 unigram features. The 1st token `brexit` has appeared twice in the first title and once in the second title."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find (1) the most friquent token in a document, (2) the number of times it appears in that document \n",
    "# and (3) the document in which it appears\n",
    "maximum = vector.toarray().max()\n",
    "index_of_maximum = np.where(vector.toarray() == maximum)\n",
    "\n",
    "print(\"max:\", maximum)\n",
    "print(\"index:\", index_of_maximum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort the vector of integer count in ascending order\n",
    "np.sort(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example using the entire data set of News Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus['text'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert a collection of text documents to a matrix of token counts\n",
    "\n",
    "vectorizer_corpus = CountVectorizer()\n",
    "\n",
    "#  Learn the vocabulary from the corpus and tokenise\n",
    "vectorizer_corpus.fit(corpus['text'])\n",
    "\n",
    "# Transform documents to document-term matrix\n",
    "vector_corpus = vectorizer_corpus.transform(corpus['text'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using the vocabulary_ attribute\n",
    "print(dict(sorted(vectorizer_corpus.vocabulary_.items(), key=lambda item: item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the document-term matrix \n",
    "print(vector_corpus.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dimensions of vector_corpus.toarray(), i.e., number of rows and columns\n",
    "vector_corpus.toarray().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1\n",
    "\n",
    "Using the entire corpus, find (1) the most friquent token in a document, (2) the number of times it appears in that document and (3) the document in which it appears."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please write below the code for Exercise 1\n",
    "\n",
    "maximum = vector_corpus.toarray().max()\n",
    "index_of_maximum = np.where(vector_corpus.toarray() == maximum)\n",
    "\n",
    "print(\"max:\", maximum)\n",
    "print(\"token index:\", index_of_maximum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The the most frequent token is in document 3 and indexed 12823. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the token indexed 12823 by getting a key in a dictionary by its value \n",
    "# The value in the \"vectorizer_corpus.vocabulary_\" is the token index\n",
    "\n",
    "dict((v,k) for k,v in vectorizer_corpus.vocabulary_.items())[12823]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To double check, get value by key\n",
    "\n",
    "vectorizer_corpus.vocabulary_.get('the')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bi-grams (combination of two tokens)\n",
    "In the unigram transformation, each token is a feature. For example, `general` and `election` are two separate features. The bi-gram transformation relaxes this contrain by pairing each word to previous and subsequent words.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extracting unigrams and bigrams\n",
    "    # ngram_range of (1, 1) extracts unigrams\n",
    "    # ngram_range of (1, 2) extracts unigrams and bigrams\n",
    "    # ngram_range of (1, 2) extracts bigrams\n",
    "\n",
    "# Create an instance of the CountVectorizer class set bigram extraction   \n",
    "vectorizer = CountVectorizer(ngram_range=(2,2))\n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "vectorizer.fit(corpus_toy['title'])\n",
    "\n",
    "# Transform documents to document-term matrix\n",
    "vector = vectorizer.transform(corpus_toy['title'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using vocabulary_\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "\n",
    "# Print the document-term matrix \n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output consists of 28 bigram-based features. The count is either 1 or 0 for each of our bigram.     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Term frequency–inverse document frequency (TF-IDF)\n",
    "\n",
    "TF-IDF vectorisation weights down tokens that are present across many documents in the corpus (in particular, words like \"of\" and \"the\" if stop words are not removed) and are therefore less informative than tokens that are present in specific documents in the corpus. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Toy example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(norm=None)\n",
    "\n",
    "# Learn the vocabulary from the corpus and tokenise\n",
    "matrix = vectorizer.fit_transform(corpus_toy['title'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using vocabulary_\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))\n",
    "\n",
    "# Print the IDF scores \n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The above computes the `IDF` part. Let's get the `TF` (term frequency) as before (for illustrative purposes only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the CountVectorizer function we used above to count n-grams\n",
    "vectorizer = CountVectorizer()\n",
    "vectorizer.fit(corpus_toy['title'])\n",
    "vector = vectorizer.transform(corpus_toy['title'])\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Below we get the TF-IDF for our toy corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the TF-IDF matrix into a DataFrame\n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How is TF-IDF computed by `scikit-learn`?  \n",
    "\n",
    "\n",
    "TF-IDF(t,d) = TF * IDF\n",
    "\n",
    "What is the TF-IDF of the term 'brexit' which is term 1 in document 0 so TF-IDF(1,0)\n",
    "\n",
    "TF = 2\n",
    "\n",
    "IDF = log(N + 1 / n + 1) + 1 where N is the total number of documents and n is the number of documents in which the term appears; constant “1” is added to the numerator and denominator to prevent zero divisions (see [scikit-learn](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfTransformer.html)). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math as m\n",
    "# the term \"brexit\" is present in two of two documents\n",
    "IDF = m.log((2+1)/(2+1))+1 \n",
    "IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So TF-IDF for term 1 (brexit) in document 0 is **TF-IDF (1,0) = TF * TDF = 2 * 1 = 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's try another example, the fourth term ('election') in document 0\n",
    "\n",
    "TF-IDF(4.0) = TF * IDF\n",
    "\n",
    "TF = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the term \"election\" is present in one of two documents\n",
    "IDF = m.log((2+1)/(1+1))+1\n",
    "IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So TF-IDF for term 4 ('election') in document 0 is **TF-IDF (4,0) = TF * TDF = 1 * 1.405 = 1.405**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### The above TF-IDF matrix is not normalised. Typically, it is recommended that the TF-IDF weights are normalised meaning that the weights in the matrix will range between 0 and 1. Below is the normalisation code (L2 normalisation is default in the TfidfVectorizer function but we indicate it below for clarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert a collection of raw documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer(norm ='l2')\n",
    "\n",
    "# # Learn the vocabulary from the corpus and create a document-term matrix\n",
    "matrix = vectorizer.fit_transform(corpus_toy['title'])\n",
    "\n",
    "pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF vectorisation of the `row` news sub-corpus related to Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert our corpus of row documents to a matrix of TF-IDF features\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Learn the vocabulary from the corpus and create a document-term matrix\n",
    "matrix = vectorizer.fit_transform(corpus_brexit['text'])\n",
    "\n",
    "# Print the tokens as a dictionary with tokens (keys) and integer feature indices (values) using vocabulary_\n",
    "print(dict(sorted(vectorizer.vocabulary_.items(), key=lambda item: item[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the IDF scores\n",
    "print(vectorizer.idf_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IDF of a few tokens in the brexit corpus\n",
    "print(\"IDF score of the term 'the':\",vectorizer.idf_[vectorizer.vocabulary_[\"the\"]])\n",
    "print(\"IDF score of the term 'brexit':\",vectorizer.idf_[vectorizer.vocabulary_[\"brexit\"]])\n",
    "print(\"IDF score of the term 'deal':\",vectorizer.idf_[vectorizer.vocabulary_[\"deal\"]])\n",
    "print(\"IDF score of the term 'protesters':\", vectorizer.idf_[vectorizer.vocabulary_[\"protesters\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The word `\"the\"` is present in many documents and hence the vector value is close to 1; Converseley, the term `\"protesters\"` is present in few documents and has a higher IDF value. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF matrix\n",
    "# The vectorizer.get_feature_names() gives you the list of feature names\n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF of the token \"the\" in the brexit corpus\n",
    "tf_idf_df.loc[:,['the','brexit','deal','protesters']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The token `\"the\"` is downweighted but still has high TF-IDF weights due to the high term frequency (Note that the TF-IDF score is a product of term frequency & inverse document frequency). The term `\"protesters\"` is present in a few documents and because it's term frequency is 0 in many documents, the TF-IDF score is 0 too. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's explore some parameters of the TfidfVectorizer function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Play with the following TfidfVectorizer parameters (use Shift + Tab to explore the parameters in your notebook):\n",
    "    # stop_words='english' ; stop_words: removes stopwords, only for english, some with issues; automatically filters stop words based on intra corpus document frequency of terms \n",
    "    # min_df = e.g., 0.2; float or int, default=1.0. ignores terms that have a document frequency lower than the given threshold\n",
    "    # max_df = e.g., 0.9; float or int, default=1.0. ignores terms that have a document frequency higher than the given threshold\n",
    "    # max_features= e.g., 5\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TF-IDF vectorisation using the `tokenised` News sub-corpus related to Brexit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF on your tokenised news corpus related to Brexit\n",
    "            \n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             min_df = 0.2, # threshold depends on corpus and question\n",
    "                             max_df = 0.9) # threshold depends on corpus and question\n",
    "\n",
    "# # Learn the vocabulary from the corpus and create a document-term matrix\n",
    "matrix = vectorizer.fit_transform(corpus_brexit['tokens'])\n",
    "\n",
    "# Create a DataFrame \n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below the word `\"the\"` appears in more than 90% of the documents and is removed on that basis. Also, the word `\"protesters\"` appears in less than 20% of the documents and is removed on that basis.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the TF-IDF vectors for a few tokens\n",
    "tf_idf_df.loc[:,['the','brexit','deal','protesters']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show only tokens that are in the tf_idf_df DataFrame\n",
    "tf_idf_df.loc[:,['brexit','deal']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot two features using a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = tf_idf_df.plot(kind='scatter', x='brexit', y='deal', alpha=0.2, s=200)\n",
    "ax.set_xlabel(\"brexit\")\n",
    "ax.set_ylabel(\"deal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cluster the 25 docuemtns about Brexit using K-means clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute K-means clustering \n",
    "# See https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n",
    "from sklearn.cluster import KMeans\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assign a document to a category \n",
    "tf_idf_df['category'] = km.labels_\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plot the 3 clusters using a scatter plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify a color for each category\n",
    "colormap = {\n",
    "    0: 'red',\n",
    "    1: 'green',\n",
    "    2: 'blue'\n",
    "}\n",
    "\n",
    "# Create a color map\n",
    "colors = tf_idf_df.apply(lambda row: colormap[row.category], axis=1)\n",
    "\n",
    "# # Plot your scatter plot\n",
    "ax = tf_idf_df.plot(kind='scatter', x='brexit', y='deal', alpha=0.1, s=300, c=colors)\n",
    "ax.set_xlabel(\"brexit\")\n",
    "ax.set_ylabel(\"deal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster the terms `brexit` and `deal` using TF-IDF for the entire corpus  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF\n",
    "vectorizer = TfidfVectorizer(stop_words='english', \n",
    "                             min_df = 0.1, \n",
    "                             max_df = 0.9, # threshold depends on corpus and question\n",
    "                             max_features=100) \n",
    "matrix = vectorizer.fit_transform(corpus['tokens'])\n",
    "\n",
    "# DataFrame\n",
    "tf_idf_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "tf_idf_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster with 3 categories\n",
    "# Use only the terms 'brexit' and 'deal'\n",
    "km = KMeans(n_clusters=3)\n",
    "km.fit(tf_idf_df[['brexit', 'deal']])\n",
    "\n",
    "# Assign the category to the dataframe\n",
    "tf_idf_df['category'] = km.labels_\n",
    "\n",
    "# Create a color map\n",
    "colormap = { 0: 'red', 1: 'green', 2: 'blue' }\n",
    "colors = tf_idf_df.apply(lambda row: colormap[row.category], axis=1)\n",
    "\n",
    "# Plot your scatter plot\n",
    "ax = tf_idf_df.plot(kind='scatter', x='brexit', y='deal', alpha=0.1, s=300, c=colors)\n",
    "ax.set_xlabel(\"brexit\")\n",
    "ax.set_ylabel(\"deal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings and word2vec\n",
    "\n",
    "> You shall know a word by the company it keeps (Firth, 1957).\n",
    "\n",
    "`Word2vec` [Mikolov et al., 2013](https://arxiv.org/abs/1301.3781) and related techniques (e.g., [GloVe](https://nlp.stanford.edu/projects/glove/)) use the context of a given word — i.e., the words surrounding a word — to learn its meaning and represent it as vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two word2vec models: Skip-Gram and Continuous Bag of Words (CBOW)\n",
    "\n",
    "The skip-gram model predicts the probabilities of a word given the context of word or words. For example, in the sentence \"UK agrees Brexit trade deal\", we have a target word and context words surrounding the target word. The number of words to be considered around the target word is called the window size. Using a window size of 2, here are the first three target and context variables for the sentence \"UK agrees Brexit trade deal with EU\": \n",
    "\n",
    "| Target word | Context word(s) |\n",
    "|---|--------|\n",
    "| UK | agree Brexit |\n",
    "| agree | UK Brexit trade |\n",
    "| Brexit | UK agree trade deal  |\n",
    "\n",
    "See Akshay Kulkarni and Adarsha Shivananda. 2019. Natural Language Processing Recipes. [Chapter 3: Converting Text to Features](https://learning.oreilly.com/library/view/natural-language-processing/9781484242674/html/475440_1_En_3_Chapter.xhtml#)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert your tokens in the News dataset into a list\n",
    "corpus_brexit['tokens']= corpus_brexit['tokens'].apply(lambda token_string: token_string.split('|*|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the word2vec skip-gram model\n",
    "skipgram = Word2Vec(corpus_brexit['tokens'], size =300, window = 3, min_count=1,sg = 1)\n",
    "\n",
    "print(\"Dimensionality—size of vocabulary and size of vectors:\", skipgram)\n",
    "\n",
    "# access vector for one word, \"brexit\" in this instance\n",
    "print(\"vectors for 'brexit':\", skipgram['brexit'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.wv.similarity('brexit', 'migration')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "skipgram.wv.most_similar(positive = \"brexit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit Principal component analysis (PCA) on the skipgram model output and plot the first 2 components\n",
    "\n",
    "data = skipgram[skipgram.wv.vocab]\n",
    "pca = PCA(n_components=2)\n",
    "result = pca.fit_transform(data)\n",
    "# create a scatter plot of the projection\n",
    "plt.figure(figsize=(28,20))\n",
    "plt.scatter(result[:, 0], result[:, 1])\n",
    "words = list(skipgram.wv.vocab)\n",
    "\n",
    "for i, word in enumerate(words):\n",
    "       plt.annotate(word, xy=(result[i, 0], result[i, 1]))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acknowledgements\n",
    "\n",
    "1. [Converting Text to Features,](https://learning.oreilly.com/library/view/natural-language-processing/9781484242674/html/475440_1_En_3_Chapter.xhtml#) in _Natural Language Processing Recipes_. Akshay Kulkarni & Adarsha Shivananda. 2019.\n",
    "2. [Sklearn's module on feature extraction](https://scikit-learn.org/stable/modules/feature_extraction.html).\n",
    "3. [Vector Semantics and Embeddings,](https://web.stanford.edu/~jurafsky/slp3/6.pdf) in _Speech and Language Processing_. Daniel Jurafsky & James H. Martin. Draft of December 30, 2020.\n",
    "4. [K-Means Clustering with scikit-learn.](http://jonathansoma.com/lede/algorithms-2017/classes/clustering/k-means-clustering-with-scikit-learn/)\n",
    "5. [Pandas for Everyone.](https://www.pearson.com/us/higher-education/program/Chen-Pandas-for-Everyone-Python-Data-Analysis/PGM335102.html). Daniel Chen. 2018. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
